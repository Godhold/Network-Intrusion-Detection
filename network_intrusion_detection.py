# -*- coding: utf-8 -*-
"""Network Intrusion Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pNsDPWY4yUvbtBjDX6ubSocckCSfD-gY

**Data Loading and Exploration**
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM
from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import roc_curve, auc
import joblib
from google.colab import drive
drive.mount('/content/drive')

dataset=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/networkDataset.csv')
dataset.info()

dataset.head()

dataset.describe()

dataset.describe(include='object')

labels = dataset["class"].value_counts().index
sizes = dataset["class"].value_counts().values

plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=sns.color_palette("coolwarm"))
plt.axis('equal')
plt.show()

categories = ['protocol_type', 'service', 'flag']
fig, axes = plt.subplots(nrows=1, ncols=len(categories), figsize=(15, 5))

for i, category in enumerate(categories):
    sns.countplot(x=category, data=dataset, hue='class', palette=("coolwarm"), ax=axes[i])
    axes[i].set_title(f'Distribution of {category}')
    axes[i].tick_params(axis='x', rotation=45)

plt.show()

dataset.isnull().sum()

dataset.dropna(inplace=True)

# duplicates
print(f"Number of duplicate rows in train: {dataset.duplicated().sum()}")

from sklearn.preprocessing import LabelEncoder

# Identify object-type columns
object_columns = dataset.select_dtypes(include='object').columns

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Label encode each object-type column
for column in object_columns:
    dataset[column] = label_encoder.fit_transform(dataset[column])

# Display the updated dataset
dataset.head()

"""**Data Preprocessing and Splitting**"""

# Separating features and target variable
X = dataset.drop('class', axis=1)
y = dataset['class']

# Using mutual information to select the top k features
k_best = 15  # You can adjust this based on your preferences
selector = SelectKBest(score_func=mutual_info_classif, k=k_best)
X_selected = selector.fit_transform(X, y)

# Getting the selected feature indices
selected_indices = selector.get_support(indices=True)

# Display the names of the selected features
selected_features = X.columns[selected_indices]
print("Selected Features:", selected_features)

# Plot a correlation matrix for the selected features
selected_features_with_target = list(selected_features) + ['class']
selected_data = dataset[selected_features_with_target]
correlation_matrix = selected_data.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=.5)
plt.title("Correlation Matrix of Selected Features")
plt.show()

selected_data.info()

selected_columns = ['service', 'flag', 'src_bytes', 'dst_bytes', 'logged_in', 'count',
                    'serror_rate', 'srv_serror_rate', 'same_srv_rate', 'diff_srv_rate',
                    'dst_host_srv_count', 'dst_host_same_srv_rate',
                    'dst_host_diff_srv_rate', 'dst_host_serror_rate',
                    'dst_host_srv_serror_rate','class']

# Select the columns from the dataset
selected_data = dataset[selected_columns]

X = selected_data.drop(['class'], axis=1)
y = selected_data['class']

"""**Model Building and Training**"""

# Define features (X) and target variable (y)
X = selected_data.drop(['class'], axis=1)
y = selected_data['class']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting sets
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform it
X_train_scaled = scaler.fit_transform(X_train)

# Transform the test data using the same scaler
X_test_scaled = scaler.transform(X_test)

X_train_scaled

# Define a function to evaluate the model
def evaluate_model(model, X_test, y_test, model_name):
    # Make predictions
    y_pred = model.predict(X_test)
    y_pred_binary = (y_pred > 0.5).astype(int)

    # Classification Report
    print(f"\nClassification Report for {model_name}:\n")
    print(classification_report(y_test, y_pred_binary))

    # Accuracy
    accuracy = accuracy_score(y_test, y_pred_binary)
    print(f"Accuracy for {model_name}: {accuracy:.4f}")

    # F1 Score
    f1 = f1_score(y_test, y_pred_binary)
    print(f"F1 Score for {model_name}: {f1:.4f}")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred_binary)
    print(f"\nConfusion Matrix for {model_name}:\n")
    print(cm)

# Define a function to plot training history
def plot_history(history, model_name):
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(f'{model_name} Training Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# Model 1: Basic Neural Network
model1 = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history1 = model1.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))

# Evaluate Model 1
evaluate_model(model1, X_test_scaled, y_test, 'Model 1')

# Model 2: Convolutional Neural Network
X_train_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

model2 = Sequential([
    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history2 = model2.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_cnn, y_test))

# Evaluate Model 2
evaluate_model(model2, X_test_cnn, y_test, 'Model 2')

# Model 3: Recurrent Neural Network (LSTM)
X_train_rnn = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_rnn = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

model3 = Sequential([
    LSTM(64, activation='relu', input_shape=(1, X_train_scaled.shape[1])),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history3 = model3.fit(X_train_rnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_rnn, y_test))

# Evaluate Model 3
evaluate_model(model3, X_test_rnn, y_test, 'Model 3')

# Plot training history
plot_history(history1, 'Model 1')
plot_history(history2, 'Model 2')
plot_history(history3, 'Model 3')

# Define a function to plot AUC-ROC curve
def plot_auc(model, X_test, y_test, model_name):
    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate ROC curve and AUC
    fpr, tpr, _ = roc_curve(y_test, y_pred)
    roc_auc = auc(fpr, tpr)

    # Plot ROC curve
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'{model_name} ROC Curve')
    plt.legend(loc='lower right')
    plt.show()

# Plot AUC-ROC curve for each model
plot_auc(model1, X_test_scaled, y_test, 'Model 1')
plot_auc(model2, X_test_cnn, y_test, 'Model 2')
plot_auc(model3, X_test_rnn, y_test, 'Model 3')

# Save Model 3 and the scaler
model3.save('/content/drive/MyDrive/Colab Notebooks/model3.h5')

# Save the scaler
joblib.dump(scaler, '/content/drive/MyDrive/Colab Notebooks/scaler.pkl')